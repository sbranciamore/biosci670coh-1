{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github",
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/cohmathonc/biosci670/blob/master/IntroductionComputationalMethods/03_IntroCompMethods_NumericalDifferentiation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yX3LnQf9MyIt",
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aGaSVwOa5imw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Numerical Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0tEMzJC-DU_h",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The continuous derivative of an analytic function $f(x)$ is defined as the limit\n",
    "\n",
    "$$f'(x)=\\lim_{h\\rightarrow 0}\\frac{f(x+h)-f(x)}{h} \\; , \\tag{1}$$\n",
    "\n",
    "where $h$ is in infinitesimal small quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the discussion of numeric precision (see [this notebook](https://github.com/cohmathonc/biosci670/blob/master/IntroductionComputationalMethods/02_IntroPythonForScientificComputing.ipynb)), we saw that *any*  digital number representation is discrete.\n",
    "This implies that $h$ is finite and we cannot compute the limit $h\\rightarrow 0$. \n",
    "\n",
    "Often we have to resort to numerical differentiation in cases when the functional expression is unknown, which means that $f(x)$ cannot be evaluated at any arbitrary point $x$ or $x+h$.\n",
    "\n",
    "The goal of numerical differentiation is to approximate the n-th derivative, given a sequence of values $f(x_0), f(x_1), \\ldots f(x_N)$ of an unkown function $f(x)$ at known evaluation points $x_0, x_1, \\ldots, x_N$. \n",
    "Often the points at which values of a function are known are equally spaced. In that case, the spacing $h$ corresponds to the spacing between subsequent points, i.e. $h=x_{i+1} - x_i = \\Delta x$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## domain\n",
    "x_min = 0\n",
    "x_max = 4*np.pi\n",
    "\n",
    "# we can either \n",
    "\n",
    "# (1) fix the number of points, or\n",
    "# n_points = 100\n",
    "# x = np.linspace(x_min, x_max, n_points)\n",
    "# h = (x_max-x_min)/(n_points-1)\n",
    "\n",
    "# (2) fix the grid spacing\n",
    "delta_x = 0.1\n",
    "x = np.arange(x_min, x_max, delta_x)\n",
    "n_points = len(x)\n",
    "\n",
    "## function values\n",
    "y = np.cos(x)\n",
    "\n",
    "plt.plot(x, y, label='cos(x)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discretization**\n",
    "\n",
    "<img src=\"figs/array1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oY-V9ugh7Tfb",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forward Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Identifying $h$ in Eq. (1) with $\\Delta x$, we can approximate the derivative of the function at a specific point $x_i$ by **finite differences**:\n",
    "\n",
    "$$f'(x_i)\\approx \\frac{f(x_i+\\Delta x)-f(x_i)}{\\Delta x} = \\frac{f(x_{i+1})-f(x_i)}{\\Delta x}\\;. \\tag{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The formula in Eq. (2) is called the **forward difference formula** because it estimates the function's derivative at $x_i$ using information about the function's value at the point $x_i$ and the 'next' point $x_{i+1}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "- Approximate the derivative of $\\cos(x)$ using the forward difference formula (2). \n",
    "- Plot $\\cos(x)$, $\\cos'(x)=-\\sin(x)$ and the numerical approximation.\n",
    "- Examine the behavior of the numerical approximation for varying size of the grid spacing $\\Delta x$. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def derivative_forward(y, delta_x):\n",
    "    \"\"\"\n",
    "    Computes forward difference on elements in 'y' array, assuming constant grid spacing 'delta_x'.\n",
    "\n",
    "    Args:\n",
    "    - y: an array of data points / function values\n",
    "    - x: scalar, indicating the spacing between subsequent observation/evaluation points\n",
    "    \"\"\"\n",
    "    n_eval_points = y.shape[0]\n",
    "    derivative = np.ones(n_eval_points) * np.nan\n",
    "    for i in range(n_eval_points-1):\n",
    "        derivative[i] = (y[i+1]-y[i])/delta_x\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation \n",
    "- creates a new array $y'$ (`derivative`)\n",
    "- scans the input array `y`, and for each index $i$ of `y` computes the finite difference by accessing the $i$-th and $(i+1)$-th element of `y`.\n",
    "- assigns the finite difference to the $i$-th element of vector $y'$ (`derivative`)\n",
    "\n",
    "<img src=\"figs/array2.png\">\n",
    "\n",
    "Note that the last element ($i=N$) of the array $y'$ (`derivative`) remains empty because the operation `y[i+1]-y[i]` can only be performed for $i\\leq N-1$.\n",
    "\n",
    "We have multiple options to handle this in our implementation. We could:\n",
    "1. Create an array `derivative` that has one element less than the array `y`.\n",
    "2. Create an array `derivative` of same size as array `y` and explicitly indicate that no specific value is assigned to the last element.\n",
    "\n",
    "Above implementation uses approach (2) by initializing the array `derivative` with elements of the `np.nan` data type ([Not a Number](https://en.wikipedia.org/wiki/NaN)) instead of a specific value, such as ones or zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS IS EQUIVALENT TO ABOVE BUT AVOIDS FOR LOOP BY VECTORIZATION\n",
    "\n",
    "def derivative_forward_array(y, delta_x):\n",
    "    \"\"\"\n",
    "    Computes forward difference on elements in 'y' array, assuming constant grid spacing 'delta_x'.\n",
    "\n",
    "    Args:\n",
    "    - y: an array of data points / function values\n",
    "    - x: scalar, indicating the spacing between subsequent observation/evaluation points\n",
    "    \"\"\"\n",
    "    # instead of iterating over each pair i, i+1, we create a new shifted array\n",
    "    y_forward = np.roll(y, -1)     # i-th item in this array corresponds to value at i+1 in y array\n",
    "    derivative = (y_forward - y)/delta_x\n",
    "    derivative[-1] = np.nan\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Domain\n",
    "x_min = 0\n",
    "x_max = 4*np.pi\n",
    "\n",
    "## Evaluation Grid\n",
    "delta_x = 0.5\n",
    "x = np.arange(x_min, x_max, delta_x)\n",
    "\n",
    "## function values\n",
    "y = np.cos(x)\n",
    "y_p = -np.sin(x)                                  # analytic derivative\n",
    "y_p_approx = derivative_forward(y, delta_x)       # numeric approximation of derivative\n",
    "\n",
    "\n",
    "# plot f, f', f'_approximated\n",
    "fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "ax = fig.add_subplot(111) \n",
    "ax.plot(x, y, \n",
    "        label='f(x) = cos(x)' )\n",
    "ax.plot(x, y_p, \n",
    "        label=\"exact f'(x) = -sin(x)\")\n",
    "ax.plot(x, y_p_approx, \n",
    "        label=\"approx f'(x)\", color='red', linestyle=':')\n",
    "\n",
    "# change x axis ticks to multiples of pi\n",
    "x_ticks_values = np.arange(0, 4*np.pi, np.pi/2 )\n",
    "x_ticks_labels = np.arange(0, 4, 0.5)\n",
    "ax.set_xticks(x_ticks_values)\n",
    "ax.set_xticklabels(x_ticks_labels)\n",
    "ax.set_xlabel(\"x in multiples of $\\pi$\")\n",
    "\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GU4ySeJAzDjz",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sources of Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Above example shows that the numerical values of the derivative computed by approximation (2) deviate from the analytic derivative with increasing $\\Delta x$.\n",
    "\n",
    "To estimate the error made by this approximation we focus on a specific point $x_0$ and expand the function $f(x)$ as a [*Taylor Series*](https://en.wikipedia.org/wiki/Taylor_series) around that point.\n",
    "A Taylor series is a representation of a function at a single point $x_0$ as an infinite sum of terms that are calculated from the values of the function's derivatives at this point:\n",
    "\n",
    "$$f(x)\\big|_{x=x_0} = \\sum_{n=0}^\\infty\\frac{f^{(n)}(x_0)}{n\\,!}(x-x_0)^n \\; . \\tag{3}$$\n",
    "\n",
    "Only taking into account the 0th ($n=0$), 1st ($n=1$) and 2nd ($n=2$) derivatives, we obtain the following approximation:\n",
    "$$f(x)\\big|_{x=x_0} \\approx f(x_0) +(x-x_0)\\, f'(x_0) + \\frac{(x-x_0)^2}{2}f''(x_0) \\;. \\tag{4}$$\n",
    "\n",
    "Where we neglected terms of third order and higher in $(x-x_0)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does this help us in estimating the approximation error in the *forward difference formula* (2) introduced above?\n",
    "\n",
    "Substituting $h=x-x_0$ in the truncated Taylor expansion (4) gives\n",
    "$$f(x_0+h) \\approx  f(x_0) + h\\,f'(x_0) + \\frac{h^2}{2}f''(x_0) ;.$$\n",
    "\n",
    "We can now bring $f'(x_0)$ to the left-hand side and recover the *forward difference formula* from before (2):\n",
    "    $$f'(x_0)\\approx \\frac{f(x_0+h)-f(x_0)}{h} -  \\frac{h}{2}f''(x_0)\\;. $$\n",
    "    \n",
    "Thus, the error made by this approximation is:\n",
    "$$\\underbrace{\\frac{f(x_0+h)-f(x_0)}{h}}_{\\text{forward difference approximation} } - f'(x_0)\\approx   \\underbrace{\\frac{h}{2}f''(x_0)}_{\\text{truncation error} } \\;. \\tag{5}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This error is called **truncation error** because the *forward difference formula* is obtained by truncating this term from the *actual* derivative, see Eq. (3). \n",
    "For the *forward difference formula*, the truncation error  is proportional to $h=h^1$ ($\\mathcal{O}(h)$)\n",
    "The *forward difference formula* (2) is therefore called a **first order method**.\n",
    "\n",
    "*Note 1*:\n",
    "In step from equation (3) to (4), we have truncated multiple terms of higher order in $h$ (all terms of order $n\\geq3$). Since we assume that $h$ is very small ($h<<1$), it follows that $h^n>h^{n+1}$. The most important error term is therefore the one of lowest order in $h$ and we can neglect all other terms.\n",
    "\n",
    "*Note 2*:\n",
    "The truncation error is proportional to the second derivative (and higher derivatives) of the function whose first derivative we aim to approximate. \n",
    "This means that the forward difference formula is the *exact* derivative for linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7VPcJpbA9xtU",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "---\n",
    "**Exercise:**\n",
    "\n",
    "Let's estimate the  error made by the  *forward difference* approximation (2) and compare it with the theoretical truncation error.\n",
    "\n",
    "For this example we consider the function $f(x)=\\cos(x)$ with first derivative $f'(x)=-\\sin(x)$ and second derivative $f''(x)=-\\cos(x)$. \n",
    "\n",
    "We approximate its derivative $f'_{\\mathrm{num}}(x)$ at the point $x_0=\\pi/6$ and compute the following errors:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{approximation error:}  \\qquad\\epsilon_{\\mathrm{num}} &= \\bigl| f'(x_0) - f'_{\\mathrm{num}}(x_0) \\bigr| \\\\\n",
    "\\text{truncation error:}  \\qquad\\epsilon_{\\mathrm{trun}} &= -\\frac{h}{2}\\, f''(x_0) = \\frac{h}{2}\\cos(x_0)\n",
    "\\end{align}\n",
    "\n",
    "We repeat this computation for multiple values of $h$, starting from $h=0.1$ and halving its value in each subsequent evaluation, so that $h_i =\\frac{0.1}{2^i}$ for $i\\in [0, 1 ...,40]$.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Functions and evaluation points\n",
    "# f=cos(x_0), f'=sin(x_0); x_0 = pi/6\n",
    "x_0 = np.pi/6\n",
    "y   = np.cos(x_0)\n",
    "y_p = -np.sin(x_0)\n",
    "\n",
    "## Initalization\n",
    "n_steps = 40\n",
    "inital_h = 0.1\n",
    "\n",
    "h_values = np.zeros(n_steps)                  # for h values\n",
    "y_p_approx_num = np.zeros(n_steps)            # for numeric approximation of f'\n",
    "eps_num = np.zeros(n_steps)                   # for approximation error\n",
    "eps_trun = np.zeros(n_steps)                  # for truncation error\n",
    "\n",
    "## Computations in loop\n",
    "for i in range(n_steps):\n",
    "    # compute current h\n",
    "    h_values[i] = inital_h/(2**i)\n",
    "    # compute discretization (we only consider x_0 and its neighbouring points)\n",
    "    x = np.array([x_0-h_values[i], x_0, x_0+h_values[i]])           # x_0 at index 1\n",
    "    y = np.cos(x)\n",
    "    # compute numerical approximation of cos(x_0)' with current h\n",
    "    y_p_approx_num_x0 = derivative_forward(y, h_values[i])[1]       # get approximated derivative at x_0 \n",
    "    y_p_approx_num[i] = y_p_approx_num_x0                           \n",
    "    # compute error between analytic and numeric derivative\n",
    "    eps_num[i] = np.abs(y_p - y_p_approx_num_x0)\n",
    "    # compute the theoretical (truncation) error from taylor expansion \n",
    "    eps_trun[i] = np.cos(x_0)*h_values[i]/2 \n",
    "\n",
    "## Print results\n",
    "print(\"eps_num: \",eps_num)\n",
    "print(\"eps_trun: \",eps_trun)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ppVfurzUDqg3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, we compare those two errors graphically by plotting the errors $\\epsilon_{\\mathrm{num}}$, $\\epsilon_{\\mathrm{trun}}$ against $h$. Since both quantities, errors and $h$, vary by multiple orders of magnitude, we use logarithmic axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "colab_type": "code",
    "id": "JY1diOe2_Jyg",
    "outputId": "e8c2cbca-7192-4e50-ff81-a5bbe32323bd",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "ax = fig.add_subplot(111) \n",
    "\n",
    "ax.loglog(h_values, eps_num, marker='o', \n",
    "          label='error of numerical approximation: $\\epsilon_{\\mathrm{num}}$' )\n",
    "ax.loglog(h_values, eps_trun, marker='x', \n",
    "          label='theoretical truncation error: $\\epsilon_{\\mathrm{trun}}$' )\n",
    "\n",
    "ax.set_xlabel('$h$')\n",
    "ax.set_ylabel(\"absolute error\")\n",
    "ax.set_title(\"Errors in forward-difference approximation of 1st \" \\\n",
    "             \"derivative of $\\cos(\\pi/6)$\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hYCi55pQGQCc",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For large values $h$, the error of our numerical approximation $\\epsilon_{\\mathrm{num}}$ agrees with the truncation error $\\epsilon_{\\mathrm{trun}}$ that is theoretically expected for this *first order* differentiation method.\n",
    "However, with decreasing $h$, the numerical error *increases* and begins to deviate from the expected truncation error.\n",
    "\n",
    "This increase in $\\epsilon_{\\mathrm{num}}$ is not explained by truncation but is caused by *rounding* due to the finite precision of standard numeric data types. This error is called **rounding error**.\n",
    "**Rounding** and **truncation** are the *two primary sources of errors* when using numerical methods. \n",
    "\n",
    "Such errors can have serious consequences. A few extreme cases where bad numerical computing practices lead to disasters are collected [here](http://www-users.math.umn.edu/~arnold/disasters/) and [here](https://www5.in.tum.de/persons/huckle/bugse.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8TY0NmjALpqA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aTwoQ6obDDKf",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We are concerned with numerical methods that approximate the solution $u$ of a continuous problem by computing the solution $\\tilde{u}$ of a discretized problem. \n",
    "In the previous section we have shown that the accuracy of this approximation depends on the granularity of the discretization, i.e. the spacing $h$. Therefore, we denote the numerical approximation by $\\tilde{u}_h$.\n",
    "\n",
    "An important characteristic of a numerical method is its convergence behavior, that is *whether* and *how quickly* the numerical estimate $\\tilde{u}_h$ converges to the exact value $\\tilde{u}$ with decreasing $h$.\n",
    "\n",
    "In Eq. (5), we showed that the truncation error of the *forward difference method*, Eq  (2), is expected to be proportional to $h$ and therefore the method is said to be *of order 1*.\n",
    "Decreasing $h$ by a factor of e.g. $2$ will decrease the error in the estimation $\\tilde{u}_{h/2}$ by the same factor $2$.\n",
    "\n",
    "In general, when a numerical method is said to be *of order $p$*,  this means that for sufficiently small $h$:\n",
    "$$\\left| \\tilde{u}_h-u\\right|\\leq C\\, h^p \\tag{5}\\; ,$$\n",
    "where $C$ is a number independent of $h$ that typically depends on the exact solution.\n",
    "The speed at which the approximation by this numerical method approaches the true solution is called the *convergence rate* of the method and is $h^p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If the order $p$ of a given numerical method is known, estimating $p$ from a sequence of approximations is a good way to check the implementation of this method. \n",
    "\n",
    "If the exact value $u$ is known, we can compute\n",
    "$$\\log{\\left| \\tilde{u}_h-u\\right|} \\leq \\log{\\left|C\\right|} + p\\log{h}$$\n",
    "for multiple different values $h$ and fit a linear function of $\\log{h}$ to $\\log{\\left| \\tilde{u}_h-u\\right|}$ to approximate $p$.\n",
    "\n",
    "The standard way to obtain precise estimates for $p$ is to halve the spacing $h$ and compare the ratios of the errors resulting from subsequent spacings:\n",
    "\n",
    "$$ \\frac{\\left|\\tilde{u}_{h}-u\\right|}{\\left|\\tilde{u}_{h/2}-u\\right|} \\leq \\frac{C\\, h^p }{C\\, (h/2)^p }=2^p\\tag{6}$$\n",
    "and thus\n",
    "$$\\log_2{\\left|\\frac{\\tilde{u}_{h}-u}{\\tilde{u}_{h/2}-u}\\right| \\leq p } \\tag{7}\\; .$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if the the exact value $u$ is not known, comparing the ratios of differences between $\\tilde{u}_{h}$ for different $h$ yields a similar result:\n",
    "\n",
    "$$ \\frac{\\left|\\tilde{u}_{h}-\\tilde{u}_{h/2}\\right|}{\\left|\\tilde{u}_{h/2}-\\tilde{u}_{h/4}\\right|} \\leq \\frac{1-2^{-p}}{2^{-p}-2^{-2p}}=2^p\\tag{8}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZycK2krQzD7t",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "**Exercise (2):**\n",
    "\n",
    "In the previous exercise we chose $h_{i+1} = \\frac{1}{2}h_i$, so that we can now apply formula (8) to verify whether our implementation of the *forward difference method* shows the expected convergence behavior.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "aALaoy5RDIBY",
    "outputId": "b5904139-918d-4b36-f400-3fc0c42d503f",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# compute ratio of errors between steps: \n",
    "# ratio(n) = error(n-1)/error(n)\n",
    "#          = error(h_n)/error(h_n/2)\n",
    "\n",
    "eps_ratio = np.ones_like(eps_num)*np.nan\n",
    "\n",
    "for i in range(1,eps_num.shape[0]):         # start at 1 to access index i-1\n",
    "    eps_ratio[i] = eps_num[i-1]/eps_num[i]\n",
    "\n",
    "convergence_order = np.log2(eps_ratio)\n",
    "\n",
    "print(\"ratio of subsequent error: \", eps_ratio)\n",
    "print(\"convergence order: \",convergence_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o1evwLdAHnG-",
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Other Finite Difference Schemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SzUB4zgtMAVF",
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\n",
    "We will now apply the analysis methods developed above to various finite difference schemas.\n",
    "To facilitate this analysis we define functions for recurring tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ziZTLt9EefVS",
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "   \n",
    "def compute_approx_different_h(function_num_deriv, function, \n",
    "                               inital_h=0.1, x_0=0, n_steps=40):\n",
    "    \"\"\"\n",
    "    Computes 'n_steps' approximations of derivative of 'function' around 'x_0'\n",
    "    using 'function_num_deriv', halving spacing delta x in each iteration.\n",
    "\n",
    "    Args:\n",
    "    - function_num_deriv: function object that computes numerical derivative from \n",
    "                          - array of function evaluations / data points and \n",
    "                          - spacing of evaluation points delta_x\n",
    "    - function: function object for function whose derivative is to be estimated\n",
    "    - initial_h: float, initial spacing\n",
    "    - x_0: float, point where derivative of 'function' is to be estimated\n",
    "    - n_steps: integer number of different spacings steps\n",
    "    - n: degree of derivative; only defined for first, second and third derivative.\n",
    "\n",
    "    Returns:\n",
    "    - array of spacing values, array of estimated derivatives at x_0 for each choosen spacing\n",
    "    \"\"\"\n",
    "    # initialize arrays\n",
    "    h_values = np.zeros(n_steps)                  # for h values\n",
    "    y_p_approx_num = np.zeros(n_steps)            # for numeric approximation of f'\n",
    "    # compute approximations for different h\n",
    "    for i in range(n_steps):\n",
    "        # compute current h\n",
    "        h_values[i] = inital_h/(2**i)\n",
    "        # compute discretization (we only consider x_0 and its neighbouring points)\n",
    "        x = np.array([x_0-h_values[i], x_0, x_0+h_values[i]])      # x_0 at index 1\n",
    "        y = function(x)\n",
    "        # compute numerical approximation of cos(x_0)' with current h\n",
    "        y_p_approx_num[i] = function_num_deriv(y, h_values[i])[1]  # get approximated derivative at x_0                         \n",
    "    return h_values, y_p_approx_num\n",
    "\n",
    "  \n",
    "def compute_convergence_order(eps):\n",
    "    \"\"\"\n",
    "    Estimates convergence order from ratio of errors between \n",
    "    finite difference estimations with different spacings.\n",
    "    Assumes that step size between subsequent estimations has been halved.\n",
    "    \"\"\"\n",
    "    eps_shifted = np.roll(eps, shift=1)\n",
    "    eps_shifted[0] = np.nan\n",
    "    eps_ratio = eps_shifted / eps\n",
    "    convergence_order = np.log2(eps_ratio)\n",
    "    return convergence_order\n",
    "  \n",
    "   \n",
    "def plot_errors(h_vals, eps_num, eps_trun=None, title=None):\n",
    "    \"\"\"\n",
    "    Plots errors vs stepsize on LogLog axes.\n",
    "\n",
    "    Args:\n",
    "    - eps_num: array of errors of the numeric approximation\n",
    "    - eps_trun: array of theoretic truncation errors\n",
    "    - h_vals: array of h values\n",
    "\n",
    "    All arrays are expeceted to be of equal length\n",
    "    \"\"\"\n",
    "    # plot\n",
    "    fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "    ax = fig.add_subplot(111) \n",
    "    ax.loglog(h_vals, eps_num, marker='o', \n",
    "            label='error of numerical approximation: $\\epsilon_{\\mathrm{num}}$')\n",
    "    if eps_trun is not None:\n",
    "        ax.loglog(h_vals, eps_trun, marker='x', \n",
    "              label='theoretical truncation error: $\\epsilon_{\\mathrm{trun}}$')\n",
    "    ax.set_xlabel('spacing $h$')\n",
    "    ax.set_ylabel(\"absolute error\")\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    ax.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cUzQ7T_l6PYm",
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### Backward Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QQlyG0egK8w8",
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Similary to the *forward difference* (2), we can define the *backward difference*:\n",
    "\n",
    "$$f'(x)\\approx \\frac{f(x)-f(x-h)}{h}\\, \\qquad \\text{with}\\qquad h>0 \\;. \\tag{9}$$\n",
    "\n",
    "You can easily derive this formula from (2) by assuming a negative $h$, say $h=-\\tilde{h}$ where $\\tilde{h}>0$.\n",
    "\n",
    "Like the *forward difference*, this is a *one-sided difference* first order method.\n",
    "\n",
    "---\n",
    "**Exercise (3):**\n",
    "\n",
    "Implement the backward difference method.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def derivative_backward(y, delta_x):\n",
    "    \"\"\"\n",
    "    Computes backward difference on elements in 'y' array, assuming constant grid spacing 'delta_x'.\n",
    "\n",
    "    Args:\n",
    "    - y: an array of data points / function values\n",
    "    - x: scalar, indicating the spacing between subsequent observation/evaluation points\n",
    "    \"\"\"\n",
    "    n_eval_points = y.shape[0]\n",
    "    derivative = np.ones(n_eval_points) * np.nan\n",
    "    for i in range(1,n_eval_points):\n",
    "        derivative[i] = (y[i]-y[i-1])/delta_x\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "## THIS IS EQUIVALENT TO ABOVE BUT AVOIDS FOR LOOP BY VECTORIZATION\n",
    "\n",
    "def derivative_backward_array(y, delta_x):\n",
    "    \"\"\"\n",
    "    Computes backward difference on elements in 'y' array, assuming constant grid spacing 'delta_x'.\n",
    "\n",
    "    Args:\n",
    "    - y: an array of data points / function values\n",
    "    - x: scalar, indicating the spacing between subsequent observation/evaluation points\n",
    "    \"\"\"\n",
    "    # instead of iterating over each pair i, i+1, we create a new shifted array\n",
    "    y_backward = np.roll(y, -1)     # i-th item in this array corresponds to value at i+1 in y array\n",
    "    derivative = (y_backward - y)/delta_x\n",
    "    derivative[0] = np.nan\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "## Functions and evaluation points\n",
    "x_0 = np.pi/6\n",
    "function = np.cos\n",
    "n_steps=40\n",
    "inital_h=0.1\n",
    "\n",
    "# Compute approximations\n",
    "h_values, y_p_approx_num = compute_approx_different_h(function_num_deriv=derivative_backward, \n",
    "                                                        function=function, \n",
    "                                                        x_0=x_0, n_steps=n_steps, inital_h=inital_h)\n",
    "# Compute numerical error\n",
    "y_p = -np.sin(x_0)\n",
    "eps_num = np.abs(y_p - y_p_approx_num)\n",
    "# Compute convergence order\n",
    "convergence_order  = compute_convergence_order(eps_num) \n",
    "# compute theoretical (truncation) error from taylor expansion \n",
    "eps_trun = np.cos(x_0)*h_values/2 \n",
    "# plot\n",
    "plot_errors(h_values, eps_num, eps_trun, \n",
    "            title=\"Errors of backward difference approximation of 1st derivative\")\n",
    "\n",
    "print(\"Convergence order: \",convergence_order)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L1mWWpinMGbK",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Centered Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_8235_jeX7Du",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Forward and backward difference methods are both based on *one-sided differences*.\n",
    "Could we use information from both sides of the current evaluation point $x_0$ to inform the numerical derivative?\n",
    "\n",
    "\n",
    "From the Taylor expansion, equation (3), we can derive truncated approximations for the forward and backward difference:\n",
    "\\begin{align}\n",
    " f(x+h) &= f(x) + h\\, f'(x) + \\frac{1}{2}h^2\\, f''(x) + \\frac{1}{6}h^3\\, f'''(x) \\tag{10a}\\\\\n",
    " f(x-h) &= f(x) - h\\, f'(x) + \\frac{1}{2}h^2\\, f''(x) - \\frac{1}{6}h^3\\, f'''(x) \\tag{10b}\n",
    "\\end{align}\n",
    "Substracting (10b) from (10a) gives:\n",
    "$$f(x+h) - f(x-h) = 2h\\, f'(x) + \\frac{2}{6}h^3\\, f'''(x)\\,$$\n",
    "and after rearranging:\n",
    "$$f'(x) = \\frac{f(x+h)-f(x-h)}{2h} - \\frac{1}{6}h^2\\, f'''(x)\\,. \\tag{11}$$\n",
    "\n",
    "This approximation of the derivative is called **centered difference** formula. The highest order truncation error term is of *second order* in $h$ ($\\mathcal{O}(h^2)$), which means that the *centered difference* approximation is a second order method.\n",
    "\n",
    "Higher-order approximations can be derived in a similar manner, using Taylor expansions with more terms.\n",
    "\n",
    "---\n",
    "**Exercise (4):**\n",
    "\n",
    "Implement the centered difference method for the first derivative (10) and verify that your implementation shows the expected convergence behavior.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "       \n",
    "def derivative_centered(y, delta_x):\n",
    "    \"\"\"\n",
    "    Computes centered difference on elements in 'y' array, assuming constant grid spacing 'delta_x'.\n",
    "    Args:\n",
    "    - y: an array of data points / function values\n",
    "    - x: scalar, indicating the spacing between subsequent observation/evaluation points\n",
    "    \"\"\"\n",
    "    n_eval_points = y.shape[0]\n",
    "    derivative = np.ones(n_eval_points) * np.nan\n",
    "    for i in range(1, n_eval_points-1):\n",
    "      derivative[i] = (y[i+1]-y[i-1])/(2*delta_x)\n",
    "    return derivative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "## THIS IS EQUIVALENT TO ABOVE BUT USES NUMPY'S VECTORIZATION INSTEAD OF FOR LOOP\n",
    "\n",
    "def derivative_centered_array(y, delta_x):\n",
    "    \"\"\"\n",
    "    Computes centered difference on elements in 'y' array, assuming constant grid spacing 'delta_x'.\n",
    "\n",
    "    Args:\n",
    "    - y: an array of data points / function values\n",
    "    - x: scalar, indicating the spacing between subsequent observation/evaluation points\n",
    "    \"\"\"\n",
    "    y_backward = np.roll(y, 1)     # i-th item corresponds to value at i-1 \n",
    "    y_forward = np.roll(y, -1)     # i-th item corresponds to value at i+1\n",
    "    derivative = (y_forward-y_backward)/(2*delta_x)\n",
    "    derivative[0] = np.nan          \n",
    "    derivative[-1] = np.nan\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation where you can provide either\n",
    "#  - scalar value for constant grid spacing, or\n",
    "#  - array of same size as y for actual evaluation points\n",
    "\n",
    "def derivative_centered_alternative(y, delta_x):\n",
    "    \"\"\"\n",
    "    Computes centered derivative of data at point `a` with.\n",
    "    Warning: Derivative at boundary points is not computed\n",
    "\n",
    "    Args:\n",
    "    - data: an array of data points / function values\n",
    "    - x:    can be a scalar, indicating the spacing between subsequent \n",
    "            observation/evaluation points, or\n",
    "            an array of same length as data, indicating the observation points themselves\n",
    "    \"\"\"\n",
    "    n_eval_points = data.shape[0]\n",
    "    derivative = np.ones(n_eval_points) * np.nan\n",
    "    if isinstance(x, np.ndarray):      # x is array of coordinates\n",
    "        for i in range(1, n_eval_points-1):\n",
    "            derivative[i] = (data[i+1]-data[i-1])/(x[i+1]-x[i-1])\n",
    "    elif (isinstance(x, float) or isinstance(x, int)):\n",
    "        for i in range(1, n_eval_points-1): # x is spacing between points\n",
    "            derivative[i] = (data[i+1]-data[i-1])/(2*x)\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WpH9u1pK_RQ2",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## THIS IS EQUIVALENT TO ABOVE BUT USES NUMPY'S VECTORIZATION INSTEAD OF FOR LOOP\n",
    "\n",
    "def derivative_centered_alternative_array(data, x):\n",
    "    \"\"\"\n",
    "    Returns centered derivative of function `function` at point `a` with.\n",
    "    Warning: Derivative at boundary points is not computed\n",
    "\n",
    "    Args:\n",
    "    - data: an array of data points\n",
    "    - x:    can be a scalar, indicating the spacing between subsequent \n",
    "            observation/evaluation points, or\n",
    "            an array of same length as data, indicating the observation points\n",
    "    \"\"\"\n",
    "    data_backward = np.roll(data, 1)     # i-th item corresponds to value at i-1 \n",
    "    data_forward = np.roll(data, -1)     # i-th item corresponds to value at i+1\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x_backward = np.roll(x, 1)       # i-th item corresponds to value at i-1 \n",
    "        x_forward = np.roll(x, -1)       # i-th item corresponds to value at i+1\n",
    "        derivative = (data_forward-data_backward)/(x_forward - x_backward)\n",
    "    elif (isinstance(x, float) or isinstance(x, int)):\n",
    "        derivative = (data_forward-data_backward)/(2*x)\n",
    "    # above computations use 'periodic' boundary conditions, i.e. x_0 = x_{N+1}\n",
    "    # but in general, we do not have sufficient information to approximate \n",
    "    # f' at the boundary: x_0 and x_N\n",
    "    derivative[:1] = np.nan          \n",
    "    derivative[-1:] = np.nan\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "colab_type": "code",
    "id": "HOmT0WNIO9cm",
    "outputId": "f1724c4e-1630-4c10-a407-3128d498cf22",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Functions and evaluation points\n",
    "x_0 = np.pi/6\n",
    "function = np.cos\n",
    "n_steps=40\n",
    "inital_h=0.1\n",
    "\n",
    "# Compute approximations\n",
    "h_values, y_p_approx_num = compute_approx_different_h(function_num_deriv=derivative_centered_array, \n",
    "                                                    function=function, \n",
    "                                                    x_0=x_0, n_steps=n_steps, inital_h=inital_h)\n",
    "# Compute numerical error\n",
    "y_p = -np.sin(x_0)\n",
    "eps_num = np.abs(y_p - y_p_approx_num)\n",
    "# Compute convergence order\n",
    "convergence_order  = compute_convergence_order(eps_num) \n",
    "# compute theoretical (truncation) error from taylor expansion \n",
    "eps_trun = np.sin(x_0)*h_values**2/6 \n",
    "# plot\n",
    "plot_errors(h_values, eps_num, eps_trun, \n",
    "            title=\"Errors of centered difference approximation of 1st derivative\")\n",
    "\n",
    "print(\"Convergence order: \",convergence_order)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jnjcGlA5bMrF",
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### Higher order Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hwqzf5OGMcni",
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A similar approach as for the *centered difference* scheme can be used to obtain finite approximations of higher derivatives, such as $f''(x)$, $f'''(x)$, etc.\n",
    "\n",
    "Let's derive a second order scheme for the second derivative:\n",
    "\n",
    "We add the fourth derivative term ($n=4$) to equations (10) and consider the sum of these extended (10a) and (10b):\n",
    "$$f(x+h) + f(x-h) = 2\\, f(x) + h^2\\, f''(x) + \\frac{2}{24}h^4\\, f''''(x)\\;.$$\n",
    "\n",
    "Rearrangement yields a second order finite difference approximation of the second derivative:\n",
    "$$f''(x) = \\frac{f(x+h) - 2\\,f(x) + f(x-h)}{h^2}-\\frac{2}{24}h^2\\, f''''(x) \\tag{12}$$\n",
    "\n",
    "---\n",
    "**Exercise (5):**\n",
    "\n",
    "Implement Eq. (12) and verify that your implementation shows the expected convergence behavior.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Derivatives at Domain Boundaries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qkn-M-I89WOd",
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note that our ability to approximate the derivative of a function at the domain boundary with a given finite difference scheme may be limited by the available information.\n",
    "Derivatives at these points can typically be approximated by a combination of one-sided and centered finite difference schemes.\n",
    "\n",
    "For example, the centered difference scheme (11) requires function values from two neighboring points $f(x_{i-1})$, $f(x_{i+1})$ to approximate $f'(x_i)$, and therefore cannot provide estimates for the derivative at points $x_0$ and $x_N$.\n",
    "\n",
    "On the other hand, forward and backward difference formula only require information from the current point $x_i$ and the 'next' $x_{i+1}$ (forward) or  'previous' point $x_{i-1}$ (backward), respectively.\n",
    "Hence, the first derivative on the entire grid $x_0, x_1, \\ldots, x_N$ could be estimated using a combination of forward, centered and backward difference schemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### 'Exact' Numerical Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "When the truncation error of a finite difference method vanishes for a given function, the method computes the exact derivative of that function.\n",
    "\n",
    "Consider the following polynomials:\n",
    "\n",
    "- Linear: $f(x)=1+x$\n",
    "- Quadratic: $f(x)=1+x+x^2$\n",
    "- Cubic: $f(x)=1+x+x^2+x^3$\n",
    "\n",
    "We now approximate the first derivative $f'(x)$ at $x=10$ for each of these Polynomials using the centered difference scheme and compute the approximation error in function of spacing $h$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Functions and derivatives\n",
    "def linear_function(x):\n",
    "    return 1+x\n",
    "\n",
    "def linear_function_deriv(x):\n",
    "    return 1\n",
    "\n",
    "def quad_function(x):\n",
    "    return 1+x+x**2\n",
    "\n",
    "def quad_function_deriv(x):\n",
    "    return 1+2*x\n",
    "\n",
    "def cube_function(x):\n",
    "    return 1+x+x**2+x**3\n",
    "\n",
    "def cube_function_deriv(x):\n",
    "    return 1+2*x+3*x**2\n",
    "\n",
    "\n",
    "## Evaluation points\n",
    "x_0 = 10\n",
    "n_steps=40\n",
    "inital_h=0.1\n",
    "\n",
    "# Linear\n",
    "h_values_1, y_p_approx_num_1 = compute_approx_different_h(function_num_deriv=derivative_centered_array, \n",
    "                                                        function=linear_function, \n",
    "                                                        x_0=x_0, n_steps=n_steps, inital_h=inital_h)\n",
    "y_p_1 = linear_function_deriv(x_0)\n",
    "eps_num_1 = np.abs(y_p_1 - y_p_approx_num_1)\n",
    "\n",
    "# Quadratic\n",
    "h_values_2, y_p_approx_num_2 = compute_approx_different_h(function_num_deriv=derivative_centered_array, \n",
    "                                                        function=quad_function, \n",
    "                                                        x_0=x_0, n_steps=n_steps, inital_h=inital_h)\n",
    "y_p_2 = quad_function_deriv(x_0)\n",
    "eps_num_2 = np.abs(y_p_2 - y_p_approx_num_2)\n",
    "\n",
    "# Cubic\n",
    "h_values_3, y_p_approx_num_3 = compute_approx_different_h(function_num_deriv=derivative_centered_array, \n",
    "                                                        function=cube_function, \n",
    "                                                        x_0=x_0, n_steps=n_steps, inital_h=inital_h)\n",
    "y_p_3 = cube_function_deriv(x_0)\n",
    "eps_num_3 = np.abs(y_p_3 - y_p_approx_num_3)\n",
    "\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "ax = fig.add_subplot(111) \n",
    "\n",
    "ax.loglog(h_values_1, eps_num_1, marker='o', \n",
    "        label='error of numerical approximation: Linear Function')\n",
    "ax.loglog(h_values_2, eps_num_2, marker='o', \n",
    "        label='error of numerical approximation: Quadratic Function')\n",
    "ax.loglog(h_values_3, eps_num_3, marker='o', \n",
    "        label='error of numerical approximation: Cubic Function')\n",
    "\n",
    "ax.set_xlabel('spacing $h$')\n",
    "ax.set_ylabel(\"absolute error\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The truncation error of the centered difference scheme is proportional to the third derivative of the function and therefore vanishes for linear and quadratic functions.\n",
    "The approximation error for those cases is dominated by numeric rounding errors for all choices of $h$ and increases monotonically with decreasing spacing $h$.\n",
    "This is not the case for the cubic function where the truncation error does not vanish. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sw4elk10Wkol",
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Useful Python / Numpy Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "id": "lcYSkFH9Xel0",
    "outputId": "a77af88c-009b-4ff4-d0ce-9d6c637f7b1a",
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# np.gradient(); 1st derivative, 2nd order\n",
    "# check help(np.gradient) for info\n",
    "\n",
    "x_min = 0\n",
    "x_max = 4*np.pi\n",
    "n_points = 100\n",
    "\n",
    "x = np.linspace(x_min, x_max, n_points)\n",
    "y = np.sin(x)\n",
    "y_p = np.gradient(y, x)\n",
    "\n",
    "# plot f, f'_approximated\n",
    "fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "ax = fig.add_subplot(111) \n",
    "ax.plot(x, y, \n",
    "        label='f(x) = cos(x)' )\n",
    "ax.plot(x, y_p, \n",
    "        label=\"approx f'(x)\", color='red', linestyle=':')\n",
    "\n",
    "# change x axis ticks to multiples of pi\n",
    "x_ticks_values = np.arange(0, 4*np.pi, np.pi/2 )\n",
    "x_ticks_labels = np.arange(0, 4, 0.5)\n",
    "ax.set_xticks(x_ticks_values)\n",
    "ax.set_xticklabels(x_ticks_labels)\n",
    "ax.set_xlabel(\"x in multiples of $\\pi$\")\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "id": "iw465VvRb2-3",
    "outputId": "2a756903-c5e3-43af-bb92-534c3b304bf3",
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# np.diff(x, n) computes the difference x_i+n - x_i and shortens the array by n\n",
    "\n",
    "x = np.linspace(0, 4*np.pi, 40)\n",
    "y = np.sin(x)\n",
    "\n",
    "# using np.diff, the forward difference scheme becomes\n",
    "y_p = np.diff(y, n=1) / np.diff(x, n=1)\n",
    "x_shortened = x[1:]    \n",
    "\n",
    "# plot f, f'_approximated\n",
    "fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "ax = fig.add_subplot(111) \n",
    "ax.plot(x, y, \n",
    "        label='f(x) = cos(x)' )\n",
    "\n",
    "# note that we removed x[0]\n",
    "ax.plot(x_shortened, y_p, \n",
    "        label=\"approx f'(x)\", color='red', linestyle=':')\n",
    "\n",
    "# change x axis ticks to multiples of pi\n",
    "x_ticks_values = np.arange(0, 4*np.pi, np.pi/2 )\n",
    "x_ticks_labels = np.arange(0, 4, 0.5)\n",
    "ax.set_xticks(x_ticks_values)\n",
    "ax.set_xticklabels(x_ticks_labels)\n",
    "ax.set_xlabel(\"x in multiples of $\\pi$\")\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In [this](https://github.com/cohmathonc/biosci670/blob/master/IntroductionComputationalMethods/exercises/04_NumericalDifferentiation.ipynb) exercise you extend the centered difference scheme to the domain boundary and approximate the first derivative of a sequence of given data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### About \n",
    "This notebook is part of the *biosci670* course on *Mathematical Modeling and Methods for Biomedical Science*.\n",
    "See https://github.com/cohmathonc/biosci670 for more information and material."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "L03_IntroCompMethods_NumericalDifferentiation.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
